{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Design and implement a Convolutional Neural Network (CNN) model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Imports and Device Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T05:30:56.099203Z",
     "iopub.status.busy": "2025-06-08T05:30:56.099020Z",
     "iopub.status.idle": "2025-06-08T05:31:03.771664Z",
     "shell.execute_reply": "2025-06-08T05:31:03.770824Z",
     "shell.execute_reply.started": "2025-06-08T05:30:56.099186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Transforming Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T05:31:03.773525Z",
     "iopub.status.busy": "2025-06-08T05:31:03.773205Z",
     "iopub.status.idle": "2025-06-08T05:31:03.777714Z",
     "shell.execute_reply": "2025-06-08T05:31:03.776992Z",
     "shell.execute_reply.started": "2025-06-08T05:31:03.773507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T05:31:03.781171Z",
     "iopub.status.busy": "2025-06-08T05:31:03.780933Z",
     "iopub.status.idle": "2025-06-08T05:33:23.367600Z",
     "shell.execute_reply": "2025-06-08T05:33:23.366815Z",
     "shell.execute_reply.started": "2025-06-08T05:31:03.781149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"/kaggle/input/caltech256/256_ObjectCategories\"\n",
    "full_dataset = datasets.ImageFolder(data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Test - Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T05:33:23.369444Z",
     "iopub.status.busy": "2025-06-08T05:33:23.369207Z",
     "iopub.status.idle": "2025-06-08T05:33:23.378314Z",
     "shell.execute_reply": "2025-06-08T05:33:23.377517Z",
     "shell.execute_reply.started": "2025-06-08T05:33:23.369426Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images = 30607\n",
      "Train: 21424 | Val: 4591 | Test: 4592\n"
     ]
    }
   ],
   "source": [
    "total_size = len(full_dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size   = int(0.15 * total_size)\n",
    "test_size  = total_size - train_size - val_size\n",
    "\n",
    "print(f\"Total images = {total_size}\")\n",
    "print(f\"Train: {train_size} | Val: {val_size} | Test: {test_size}\")\n",
    "\n",
    "# Split\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T05:33:23.379209Z",
     "iopub.status.busy": "2025-06-08T05:33:23.379005Z",
     "iopub.status.idle": "2025-06-08T05:33:23.387174Z",
     "shell.execute_reply": "2025-06-08T05:33:23.386639Z",
     "shell.execute_reply.started": "2025-06-08T05:33:23.379194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. VGG like CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T05:33:23.388089Z",
     "iopub.status.busy": "2025-06-08T05:33:23.387890Z",
     "iopub.status.idle": "2025-06-08T05:33:23.400727Z",
     "shell.execute_reply": "2025-06-08T05:33:23.400110Z",
     "shell.execute_reply.started": "2025-06-08T05:33:23.388073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyVGG(nn.Module):\n",
    "    def __init__(self, num_classes=257):\n",
    "        super(MyVGG, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.AdaptiveAvgPool2d((7, 7))\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T05:33:23.401709Z",
     "iopub.status.busy": "2025-06-08T05:33:23.401477Z",
     "iopub.status.idle": "2025-06-08T05:33:24.786298Z",
     "shell.execute_reply": "2025-06-08T05:33:24.785720Z",
     "shell.execute_reply.started": "2025-06-08T05:33:23.401694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = MyVGG(num_classes=257).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "num_epochs = 25\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.05,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=num_epochs,\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=10,\n",
    "    final_div_factor=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T05:33:24.787296Z",
     "iopub.status.busy": "2025-06-08T05:33:24.787026Z",
     "iopub.status.idle": "2025-06-08T08:20:57.853163Z",
     "shell.execute_reply": "2025-06-08T08:20:57.852176Z",
     "shell.execute_reply.started": "2025-06-08T05:33:24.787270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] Train Loss: 5.3324  Train Acc: 5.17%  Val Loss: 5.1915  Val Acc: 6.62%\n",
      "Epoch [2/25] Train Loss: 5.1186  Train Acc: 7.19%  Val Loss: 4.9888  Val Acc: 7.95%\n",
      "Epoch [3/25] Train Loss: 4.9095  Train Acc: 9.30%  Val Loss: 4.7202  Val Acc: 10.74%\n",
      "Epoch [4/25] Train Loss: 4.7271  Train Acc: 10.87%  Val Loss: 4.5477  Val Acc: 13.11%\n",
      "Epoch [5/25] Train Loss: 4.4747  Train Acc: 13.32%  Val Loss: 4.2782  Val Acc: 15.94%\n",
      "Epoch [6/25] Train Loss: 4.2058  Train Acc: 16.02%  Val Loss: 4.0234  Val Acc: 20.08%\n",
      "Epoch [7/25] Train Loss: 3.9459  Train Acc: 18.88%  Val Loss: 3.8128  Val Acc: 22.13%\n",
      "Epoch [8/25] Train Loss: 3.6855  Train Acc: 22.70%  Val Loss: 3.6563  Val Acc: 25.11%\n",
      "Epoch [9/25] Train Loss: 3.4530  Train Acc: 26.04%  Val Loss: 3.5358  Val Acc: 26.97%\n",
      "Epoch [10/25] Train Loss: 3.1979  Train Acc: 30.00%  Val Loss: 3.3698  Val Acc: 30.08%\n",
      "Epoch [11/25] Train Loss: 2.9716  Train Acc: 33.82%  Val Loss: 3.2003  Val Acc: 31.76%\n",
      "Epoch [12/25] Train Loss: 2.7190  Train Acc: 37.82%  Val Loss: 3.2219  Val Acc: 32.76%\n",
      "Epoch [13/25] Train Loss: 2.4867  Train Acc: 41.91%  Val Loss: 3.1636  Val Acc: 33.54%\n",
      "Epoch [14/25] Train Loss: 2.2252  Train Acc: 46.60%  Val Loss: 2.9414  Val Acc: 37.64%\n",
      "Epoch [15/25] Train Loss: 1.9497  Train Acc: 51.69%  Val Loss: 3.0580  Val Acc: 36.51%\n",
      "Epoch [16/25] Train Loss: 1.6664  Train Acc: 57.67%  Val Loss: 2.8682  Val Acc: 39.84%\n",
      "Epoch [17/25] Train Loss: 1.3352  Train Acc: 64.76%  Val Loss: 2.8988  Val Acc: 39.29%\n",
      "Epoch [18/25] Train Loss: 1.0435  Train Acc: 71.54%  Val Loss: 2.9511  Val Acc: 40.60%\n",
      "Epoch [19/25] Train Loss: 0.7884  Train Acc: 77.61%  Val Loss: 2.9551  Val Acc: 41.25%\n",
      "Epoch [20/25] Train Loss: 0.5316  Train Acc: 84.77%  Val Loss: 3.1036  Val Acc: 41.76%\n",
      "Epoch [21/25] Train Loss: 0.3580  Train Acc: 89.61%  Val Loss: 3.0804  Val Acc: 42.61%\n",
      "Epoch [22/25] Train Loss: 0.2467  Train Acc: 92.85%  Val Loss: 3.1701  Val Acc: 43.63%\n",
      "Epoch [23/25] Train Loss: 0.1770  Train Acc: 95.01%  Val Loss: 3.1821  Val Acc: 44.06%\n",
      "Epoch [24/25] Train Loss: 0.1360  Train Acc: 96.25%  Val Loss: 3.2109  Val Acc: 44.48%\n",
      "Epoch [25/25] Train Loss: 0.1274  Train Acc: 96.55%  Val Loss: 3.2282  Val Acc: 44.48%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = 100.0 * correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}  Train Acc: {train_acc:.2f}%  \"\n",
    "          f\"Val Loss: {val_loss:.4f}  Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Save the model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:20:57.854549Z",
     "iopub.status.busy": "2025-06-08T08:20:57.854247Z",
     "iopub.status.idle": "2025-06-08T08:20:58.848368Z",
     "shell.execute_reply": "2025-06-08T08:20:58.847670Z",
     "shell.execute_reply.started": "2025-06-08T08:20:57.854515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"myvgg_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 30279,
     "sourceId": 38601,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
