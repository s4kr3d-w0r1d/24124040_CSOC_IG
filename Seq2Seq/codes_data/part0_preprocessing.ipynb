{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Machine Translation(X to Y language)\n",
        "\n",
        "### Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "U8KqRhj_-p6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing necessary libraries"
      ],
      "metadata": {
        "id": "vcgXUy-l-yZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Z3fSAoru7_H5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a938e6-b624-4bfa-dcb8-863638dbf41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Loading fra-eng dataset"
      ],
      "metadata": {
        "id": "ZlLVAK0m-3ft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4fbFYqF6eHH",
        "outputId": "08ceb813-a275-4171-bffe-63d748de3713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 237,838 sentence pairs.\n"
          ]
        }
      ],
      "source": [
        "with open('fra.txt', encoding='utf-8') as f:\n",
        "    lines = f.read().strip().split('\\n')\n",
        "pairs = [line.split('\\t')[:2] for line in lines if '\\t' in line]\n",
        "print(f\"Loaded {len(pairs):,} sentence pairs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Cleaning & Truncating"
      ],
      "metadata": {
        "id": "TelcUQam-84U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(pairs)\n",
        "pairs = pairs[:200000]\n",
        "\n",
        "eng_sentences = [p[0].lower() for p in pairs]\n",
        "fra_sentences_input = ['<sos> ' + p[1].lower() for p in pairs]\n",
        "fra_sentences_target = [p[1].lower() + ' <eos>' for p in pairs]"
      ],
      "metadata": {
        "id": "-dYx2cG57jMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Tokenization"
      ],
      "metadata": {
        "id": "NbWDLIXg_OYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = Tokenizer(filters='', lower=True, oov_token='<unk>')\n",
        "fra_tokenizer = Tokenizer(filters='', lower=True, oov_token='<unk>')\n",
        "\n",
        "eng_tokenizer.fit_on_texts(eng_sentences)\n",
        "fra_tokenizer.fit_on_texts(fra_sentences_input + fra_sentences_target)  # full vocab\n",
        "\n",
        "eng_sequences = eng_tokenizer.texts_to_sequences(eng_sentences)\n",
        "fra_input_sequences = fra_tokenizer.texts_to_sequences(fra_sentences_input)\n",
        "fra_target_sequences = fra_tokenizer.texts_to_sequences(fra_sentences_target)"
      ],
      "metadata": {
        "id": "8BN4XR_V7n_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Padding"
      ],
      "metadata": {
        "id": "SDmkVUUs_RGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_eng = max(len(seq) for seq in eng_sequences)\n",
        "max_len_fra = max(max(len(seq) for seq in fra_input_sequences),\n",
        "                  max(len(seq) for seq in fra_target_sequences))\n",
        "\n",
        "encoder_input_data = pad_sequences(eng_sequences, maxlen=max_len_eng, padding='post')\n",
        "decoder_input_data = pad_sequences(fra_input_sequences, maxlen=max_len_fra, padding='post')\n",
        "decoder_target_data = pad_sequences(fra_target_sequences, maxlen=max_len_fra, padding='post')\n",
        "decoder_target_data = decoder_target_data.reshape(*decoder_target_data.shape, 1)"
      ],
      "metadata": {
        "id": "6dW4z6J07qru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Saving Tokenizers"
      ],
      "metadata": {
        "id": "kVtddv6Z_S7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('eng_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(eng_tokenizer, f)\n",
        "with open('fra_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(fra_tokenizer, f)\n",
        "\n",
        "config = {\n",
        "    'eng_vocab_size': len(eng_tokenizer.word_index) + 1,\n",
        "    'fra_vocab_size': len(fra_tokenizer.word_index) + 1,\n",
        "    'max_len_eng': max_len_eng,\n",
        "    'max_len_fra': max_len_fra,\n",
        "}\n",
        "with open('config.json', 'w') as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "np.savez_compressed(\n",
        "    'data.npz',\n",
        "    encoder_input_data=encoder_input_data,\n",
        "    decoder_input_data=decoder_input_data,\n",
        "    decoder_target_data=decoder_target_data\n",
        ")"
      ],
      "metadata": {
        "id": "9KYfYvgWhIsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Embedding Matrix"
      ],
      "metadata": {
        "id": "njkodLOrhhhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "\n",
        "glove_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in tqdm(f, desc=\"Reading GloVe\"):\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        glove_index[word] = coefs\n",
        "\n",
        "en_embedding_matrix = np.zeros((len(eng_tokenizer.word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in eng_tokenizer.word_index.items():\n",
        "    vector = glove_index.get(word)\n",
        "    if vector is not None:\n",
        "        en_embedding_matrix[i] = vector\n",
        "\n",
        "with open('en_embedding_matrix.pkl', 'wb') as f:\n",
        "    pickle.dump(en_embedding_matrix, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlARoThEhkLn",
        "outputId": "f88cf1c1-a981-4e51-c611-46a3787f8342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading GloVe: 400000it [00:09, 41631.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Checking output"
      ],
      "metadata": {
        "id": "XIL42DZb_YtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample English:\", eng_sentences[0])\n",
        "print(\"Tokenized:\", eng_sequences[0])\n",
        "print(\"Padded:\", encoder_input_data[0])\n",
        "print(\"Encoder Input Shape:\", encoder_input_data.shape)\n",
        "print(\"Decoder Input Shape:\", decoder_input_data.shape)\n",
        "print(\"Decoder Target Shape:\", decoder_target_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OxfHCHF7wy6",
        "outputId": "18a36d18-0aba-4ebb-a5d6-1560717a33c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample English: you're my type.\n",
            "Tokenized: [37, 18, 3444]\n",
            "Padded: [  37   18 3444    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "Encoder Input Shape: (200000, 55)\n",
            "Decoder Input Shape: (200000, 57)\n",
            "Decoder Target Shape: (200000, 57, 1)\n"
          ]
        }
      ]
    }
  ]
}